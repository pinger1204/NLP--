# 自然语言处理02

对自然语言处理有了大概的了解，明白NLP主要是做什么的。接下来我根据所学习的书，以及查找的资料，结合深度学习的部分内容，对自然语言处理领域中的**文本分类/情感分析**的一些内容进行记录。在之后的代码实现和模型构建中，我用到的是**Python**软件（相信大家已经装好了python），所以这里首先对接下来需要用到的库以及语料库的安装方法进行介绍。

在达到最终目的：文本分类/情感分析之前，需要对语料库进行预处理。预处理的基本步骤有：分词（Tokenization）、大小写转换、删除停用词（stop words）、删除标点符号、词干提取（stemming）、处理n-grams。

至于文本分类，就是对已有文档的自然语言进行识别，对其分类。它的应用也非常广泛，比如：垃圾过滤，新闻分类，词性标注等等。后面将提到根据情感正负，对电影评论进行分类，这是一个大致了解，在之后的介绍中，会慢慢清晰。

进行以上预处理操作，需要先加载如下代码中的库：

```python
import nltk  #导入工具包nltk
from nltk.corpus import gutenberg  #导入gutenberg语料库
from nltk import word_tokenize,sent_tokenize  
from nltk.corpus import stopwords
from nltk.stem.porter import*
from nltk.tokenize import punkt

import string

import gensim
from gensim.models.phrases import Phraser, Phrases
from gensim.models.word2vec import Word2Vec

from sklearn.manifold import TSNE

import pandas as pd
from bokeh.io import output_notebook, output_file
from bokeh.plotting import show, figure
%matplotlib inline
```

在以上代码运行成功后，就可以开始对语料库进行按步骤预处理。因此需要注意，进行分词、大小写转换等预处理步骤之前，需要运行上述代码。

#### 注意！！！

由于分词用到的库的加载有些麻烦，所以接下来对**分词**用到的工具包和语料库的载入方法进行介绍。上述代码中，分词用到的是工具包nltk和gutenberg语料库。这里的gutenberg语料库内置于nltk，需要进行下载，但是采用官方的下载方法，下载速度很慢并且容易出错，所以我这里自行下载了nltk中的语料库。

我将语料库的资料包放在百度云盘中，链接如下：https://pan.baidu.com/s/1RwK8S4bTL1xcP8-2nQMS-A 提取码：owkh。语料库下载完之后，将解压后名为nltk_data文件放到C盘根目录下，并将文件中的压缩内容全部解压，或者解压之后需要用到的内容。

同时，以上的代码还加载了其他预处理步骤用到的库，其他的库就可直接运行代码加载。但若使用的python中没有下载该库，则需要首先在**Anaconda  Prompt**下载。例如，若**gensim**需要下载则打开**Anaconda  Prompt**，输入”pip install gensim“，进行下载。具体的应用在之后用到的部分详细介绍。

