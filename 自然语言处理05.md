## 词干提取

词干提取（stemming）,就是对语料库中的单词截取到词干。比如，house和housing的词干是hous。

好处：对于较小的语料库来说，词干提取可以将含义相似的单词汇集到一个token，可以提高效率，能够更精准的定位token在词向量中的位置。

进行词干提取时，可以使用nltk中的Porter算法。首先创建一个PorterStemmer()实例，然后添加stem()方法。

```python
stemmer=PorterStemmer()  #词干提取，创建一个实例
[stemmer.stem(w.lower()) for w in gberg_sents[4]
if w.lower() not in stpwrds
```

![词干提取](https://github.com/pinger1204/NLP--/blob/master/image/%E8%AF%8D%E5%B9%B2%E6%8F%90%E5%8F%96.png)

输出结果就是经过词干提取，可以看到，有些单词已经被处理成词干形式。

需要**注意**的是，因为小语料库提供的单词较少，所以词干提取效果是不错的。而对于大语料库，它本身的单词就比较多，因此保留每个单词的时态或不同形式是有用的。



## 处理n-grams

n-grams，通俗的理解就是多元词组，例如New York这样的词，它是一个二元词组，并且通常它已经是一个固定的词组，因此将这两个单词做一个token去标记，才能准确地识别它的含义。

那么可以使用gensim库中的Phrases（）和Phraser（）方法：

1.Phrases（）用来训练一个“检测器”，来确定给定的二元分词在语料库中出现的频率（技术术语是bigram collocation）以及计算与词组中单个单词出现频率有关的得分。

2.Phraser（）获取由Phrases（）检测到的二元分词搭配，然后使用此信息创建一个可以有效地传递到语料库的对象，将所有二元分词搭配从两个连续的token转换为单个token。

```python
phrases=Phrases(gberg_sents)  
#训练检测器，确定二元分词在语料库中出现的频率及分数
bigram=Phraser(phrases) 
#获取检测到的二元分词搭配，并且创建一个可以有效传递到语料库的对象
bigram.phrasegrams
```

![计数得分](https://github.com/pinger1204/NLP--/blob/master/image/%E8%AE%A1%E6%95%B0%E5%BE%97%E5%88%86.png)

上图为处理二元分词的输出结果，（我这个结果上只显示了得分，没有分数，我也不知为何，我还在找原因）从部分结果可以看出，有的二元分词的得分很低，有的则比较高，这个得分可以用来判定是否将这两个单词作为一个词组。

在确定是否作为一个词组后，需要讨论如何使用创建的二元分词对象将两个token转换为一个。首先使用split()对有空格的字符串分解断句：

```python
tokenized_sentence = "Jon lives in New York City".split()
#分解所有有空格的字符串
tokenized_sentence  #仅输出一个字母组合列表
```

![分解短句](https://github.com/pinger1204/NLP--/blob/master/image/%E5%88%86%E8%A7%A3%E7%9F%AD%E5%8F%A5.png)

上述代码仅仅将句子根据空格分解为单词，所以输出结果如上图所示。

```python
bigram[tokenized_sentence]
```

![二元分词 token](https://github.com/pinger1204/NLP--/blob/master/image/%E4%BA%8C%E5%85%83%E5%88%86%E8%AF%8D%20token.png)

该行代码使用bigram[tokenized_sentence]将列表传递给gensim bigram对象，从输出结果也可以看出，列表包含“New_York”，这表明将“New York”转换为了一个token。

